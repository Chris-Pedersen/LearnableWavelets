{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd292200-ce60-4819-90c0-5ba7bf2a6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time, sys, os\n",
    "\n",
    "# my modules\n",
    "from sn_camels.models.models_factory import baseModelFactory, topModelFactory\n",
    "from sn_camels.models.sn_hybrid_models import sn_HybridModel\n",
    "from sn_camels.models.camels_models import model_o3_err\n",
    "from sn_camels.camels.camels_dataset import *\n",
    "\n",
    "import optuna\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e0cb2-f622-421c-bae4-8250f2a9a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Everything done within an Objective class now for optuna\n",
    "class Objective(object):\n",
    "    def __init__(self, device, seed, fmaps, fmaps_norm, fparams, batch_size, splits,\n",
    "                      arch, min_lr, beta1, beta2, epochs, monopole,\n",
    "                      num_workers, params, rot_flip_in_mem, smoothing):\n",
    "        self.device          = device\n",
    "        self.seed            = seed\n",
    "        self.fmaps          = fmaps\n",
    "        self.fmaps_norm     = fmaps_norm\n",
    "        self.fparams        = fparams\n",
    "        self.batch_size      = batch_size\n",
    "        self.splits          = splits\n",
    "        self.arch            = arch\n",
    "        self.min_lr          = min_lr\n",
    "        self.beta1           = beta1\n",
    "        self.beta2           = beta2\n",
    "        self.epochs          = epochs\n",
    "        self.monopole        = monopole\n",
    "        self.num_workers     = num_workers\n",
    "        self.params          = params\n",
    "        self.rot_flip_in_mem = rot_flip_in_mem\n",
    "        self.smoothing       = smoothing\n",
    "        print(\"Done init\")\n",
    "\n",
    "    def __call__(self,trial):\n",
    "        ## number of fields - hardcoded to 1 for now\n",
    "        channels  = 1\n",
    "\n",
    "        # tuple with the indexes of the parameters to train\n",
    "        g = self.params      #posterior mean\n",
    "        h = [6+i for i in g] #posterior variance\n",
    "\n",
    "        print(\"Suggesting trial\")\n",
    "        # get the value of the hyperparameters\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
    "        wd     = trial.suggest_float(\"wd\", 1e-8, 1e-1, log=True)\n",
    "        dr     = trial.suggest_float(\"dr\", 0.0,  0.9)\n",
    "        print(\"Suggested trial\")\n",
    "\n",
    "        print('\\nTrial number: {}'.format(trial.number))\n",
    "        print('lr: {}'.format(lr))\n",
    "        print('wd: {}'.format(wd))\n",
    "        print('dr: {}'.format(dr))\n",
    "        lr_orient = trial.suggest_float(\"lr_orient\",1e-4,1,log=True)\n",
    "        lr_scat   = trial.suggest_float(\"lr_scat\",1e-4,1,log=True)\n",
    "\n",
    "        config[\"lr_orient\"]=lr_orient\n",
    "        config[\"lr_scat\"]=lr_scat\n",
    "        print('lr_orient: {}'.format(lr_orient))\n",
    "        print('lr_scat: {}'.format(lr_scat))\n",
    "        \n",
    "        scatteringBase = baseModelFactory( #creat scattering base model\n",
    "            architecture='scattering',\n",
    "            J=2,\n",
    "            N=256,\n",
    "            M=256,\n",
    "            max_order=2,\n",
    "            initialization=\"Tight-Frame\",\n",
    "            seed=234,\n",
    "            learnable=True,\n",
    "            lr_orientation=0.005,\n",
    "            lr_scattering=0.005,\n",
    "            skip=True,\n",
    "            split_filters=True,\n",
    "            filter_video=False,\n",
    "            subsample=4,\n",
    "            device=device,\n",
    "            use_cuda=use_cuda\n",
    "        )\n",
    "        top = topModelFactory( #create cnn, mlp, linearlayer, or other\n",
    "            base=scatteringBase,\n",
    "            architecture=\"linear_layer\",\n",
    "            num_classes=sn_classes,\n",
    "            width=5,\n",
    "            average=True,\n",
    "            use_cuda=use_cuda\n",
    "        )\n",
    "\n",
    "        ## Merge these into a hybrid model\n",
    "        hybridModel = sn_HybridModel(scatteringBase=scatteringBase, top=top, use_cuda=use_cuda)\n",
    "        model=hybridModel\n",
    "        wandb.config.update({\"learnable\":scatteringBase.learnable,\n",
    "                             \"initialisation\":scatteringBase.initialization,\n",
    "                             \"wavelet seed\":scatteringBase.seed,\n",
    "                             \"learnable_parameters\":model.countLearnableParams(),\n",
    "                             \"max_order\":scatteringBase.max_order,\n",
    "                             \"skip\":scatteringBase.skip,\n",
    "                             \"split_filters\":scatteringBase.split_filters,\n",
    "                             \"subsample\":scatteringBase.subsample,\n",
    "                             \"scattering_output_dims\":scatteringBase.M_coefficient,\n",
    "                             \"n_coefficients\":scatteringBase.n_coefficients,\n",
    "                             \"top_model\":top.arch,\n",
    "                             \"spatial_average\":top.average\n",
    "                             })\n",
    "        print(\"scattering layer + cnn set up\")\n",
    "\n",
    "        ## Initialise wandb\n",
    "        wandb.login()\n",
    "        wandb.init(project=\"SN-debug-Mar8\", entity=\"chris-pedersen\",config=config)\n",
    "\n",
    "        ### LOAD DATA\n",
    "        ## camels path\n",
    "        camels_path=\"/mnt/ceph/users/camels/PUBLIC_RELEASE/CMD/2D_maps/data/\"\n",
    "\n",
    "        # get training set\n",
    "        print('\\nPreparing training set')\n",
    "        train_loader = create_dataset_multifield('train', self.seed, self.fmaps, self.fparams, self.batch_size, self.splits, self.fmaps_norm, \n",
    "                                                rot_flip_in_mem=self.rot_flip_in_mem, verbose=True)\n",
    "\n",
    "        # get validation set\n",
    "        print('\\nPreparing validation set')\n",
    "        valid_loader = create_dataset_multifield('valid', self.seed, self.fmaps, self.fparams, self.batch_size, self.splits, self.fmaps_norm, \n",
    "                                                rot_flip_in_mem=self.rot_flip_in_mem,  verbose=True)\n",
    "\n",
    "        num_train_maps=train_loader.dataset.x.size\n",
    "        wandb.config.update({\"no. training maps\": num_train_maps})\n",
    "        model.to(device=device)\n",
    "\n",
    "        # wandb\n",
    "        wandb.watch(model, log_freq=1)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "\n",
    "        print('Computing initial validation loss')\n",
    "        model.eval()\n",
    "        valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "        min_valid_loss, points = 0.0, 0\n",
    "        for x, y in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                bs   = x.shape[0]                #batch size\n",
    "                x    = x.to(device=device)       #maps\n",
    "                y    = y.to(device=device)[:,g]  #parameters\n",
    "                p    = model(x)                  #NN output\n",
    "                y_NN = p[:,g]                    #posterior mean\n",
    "                e_NN = p[:,h]                    #posterior std\n",
    "                loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "                valid_loss1 += loss1*bs\n",
    "                valid_loss2 += loss2*bs\n",
    "                points += bs\n",
    "        min_valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "        min_valid_loss = torch.mean(min_valid_loss).item()\n",
    "        print('Initial valid loss = %.3e'%min_valid_loss)\n",
    "\n",
    "        # do a loop over all epochs\n",
    "        start = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            log_dic={}\n",
    "            wave_params=hybridModel.scatteringBase.params_filters\n",
    "            orientations=wave_params[0].cpu().detach().numpy()\n",
    "            xis=wave_params[1].cpu().detach().numpy()\n",
    "            sigmas=wave_params[2].cpu().detach().numpy()\n",
    "            slants=wave_params[3].cpu().detach().numpy()\n",
    "            for aa in range(len(orientations)):\n",
    "                log_dic[\"orientation_%d\" % aa]=orientations[aa]\n",
    "                log_dic[\"xi_%d\" % aa]=xis[aa]\n",
    "                log_dic[\"sigma_%d\" % aa]=sigmas[aa]\n",
    "                log_dic[\"slant_%d\" % aa]=slants[aa]\n",
    "            # do training\n",
    "            train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "            train_loss, points = 0.0, 0\n",
    "            model.train()\n",
    "            for x, y in train_loader:\n",
    "                bs   = x.shape[0]         #batch size\n",
    "                x    = x.to(device)       #maps\n",
    "                y    = y.to(device)[:,g]  #parameters\n",
    "                p    = model(x)           #NN output\n",
    "                y_NN = p[:,g]             #posterior mean\n",
    "                e_NN = p[:,h]             #posterior std\n",
    "                loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "                train_loss1 += loss1*bs\n",
    "                train_loss2 += loss2*bs\n",
    "                points      += bs\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #if points>18000:  break\n",
    "            train_loss = torch.log(train_loss1/points) + torch.log(train_loss2/points)\n",
    "            train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "            # do validation: cosmo alone & all params\n",
    "            valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "            valid_loss, points = 0.0, 0\n",
    "            model.eval()\n",
    "            for x, y in valid_loader:\n",
    "                with torch.no_grad():\n",
    "                    bs    = x.shape[0]         #batch size\n",
    "                    x     = x.to(device)       #maps\n",
    "                    y     = y.to(device)[:,g]  #parameters\n",
    "                    p     = model(x)           #NN output\n",
    "                    y_NN  = p[:,g]             #posterior mean\n",
    "                    e_NN  = p[:,h]             #posterior std\n",
    "                    loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "                    loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                    loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "                    valid_loss1 += loss1*bs\n",
    "                    valid_loss2 += loss2*bs\n",
    "                    points     += bs\n",
    "            valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "            valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "            scheduler.step(valid_loss)\n",
    "            wandb.log({\"training loss\": train_loss, \"validation loss\": valid_loss})\n",
    "\n",
    "\n",
    "            # verbose\n",
    "            print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "            print(\"\")\n",
    "\n",
    "        stop = time.time()\n",
    "        print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "        return min_valid_loss\n",
    "\n",
    "##################################### INPUT ##########################################\n",
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n",
    "\n",
    "# architecture parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "## My stuff\n",
    "## camels path\n",
    "camels_path=\"/mnt/ceph/users/camels/PUBLIC_RELEASE/CMD/2D_maps/data/\"\n",
    "fparams    = camels_path+\"/params_IllustrisTNG.txt\"\n",
    "fmaps      = ['maps_Mcdm.npy']\n",
    "fmaps_norm = [None]\n",
    "splits     = 1\n",
    "seed       = 123\n",
    "params     = [0,1,2,3,4,5] #0(Om) 1(s8) 2(A_SN1) 3 (A_AGN1) 4(A_SN2) 5(A_AGN2)\n",
    "monopole        = True  #keep the monopole of the maps (True) or remove it (False)\n",
    "rot_flip_in_mem = False  #whether rotations and flipings are kept in memory\n",
    "smoothing  = 0  ## Smooth the maps with a Gaussian filter? 0 for no\n",
    "arch = \"sn\" ## Which model architecture to use    \n",
    "\n",
    "\n",
    "fmaps2 = camels_path+\"/Maps_Mcdm_IllustrisTNG_LH_z=0.00.npy\"\n",
    "maps  = np.load(fmaps2)\n",
    "print('Shape of the maps:',maps.shape)\n",
    "# define the array that will contain the indexes of the maps\n",
    "indexes = np.zeros(1000*splits, dtype=np.int32)\n",
    "\n",
    "# do a loop over all maps and choose the ones we want\n",
    "count = 0\n",
    "for i in range(5000):\n",
    "    if i%15 in np.arange(splits):  \n",
    "      indexes[count] = i\n",
    "      count += 1\n",
    "print('Selected %d maps out of 15000'%count)\n",
    "\n",
    "# save these maps to a new file\n",
    "maps = maps[indexes]\n",
    "np.save('maps_Mcdm.npy', maps)\n",
    "del maps\n",
    "\n",
    "## training parameters\n",
    "batch_size  = 128\n",
    "min_lr      = 1e-9\n",
    "epochs      = 100\n",
    "num_workers = 1    #number of workers to load data\n",
    "\n",
    "## Optuna params\n",
    "study_name = \"optuna/camels-test\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "n_trials=20\n",
    "\n",
    "# train networks with bayesian optimization\n",
    "objective = Objective(device, seed, fmaps, fmaps_norm, fparams, batch_size, splits,\n",
    "                      arch, min_lr, beta1, beta2, epochs, monopole, \n",
    "                    num_workers, params, rot_flip_in_mem, smoothing)\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=20)\n",
    "study = optuna.create_study(study_name=study_name, sampler=sampler, storage=storage_name,\n",
    "                            load_if_exists=True)\n",
    "study.optimize(objective, n_trials, gc_after_trial=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5cb7d-b621-4460-9403-481df3ffa27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c8ac0-9269-40fb-bc94-d1b960bc9bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet",
   "language": "python",
   "name": "wavelet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
