{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8506dce1-f8cc-48e6-ae49-7ee8959d7424",
   "metadata": {},
   "source": [
    "## Interpret\n",
    "\n",
    "Run a spatially averaged linear layer. Save the weights and convolved fields, to be plugged into the Lasso notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c49017-3021-48c9-9963-49818a1d0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time, sys, os\n",
    "import wandb\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# my modules\n",
    "from learnable_wavelets.models.models_factory import baseModelFactory, topModelFactory\n",
    "from learnable_wavelets.models.sn_hybrid_models import sn_HybridModel\n",
    "from learnable_wavelets.camels.camels_dataset import *\n",
    "from learnable_wavelets.models.camels_models import get_architecture\n",
    "from learnable_wavelets.utils.test_model import test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e916256-7069-40c5-80c5-aa3401de1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run config\n",
    "epochs=100\n",
    "lr=1e-3\n",
    "batch_size=32\n",
    "project_name=\"interp_network\"\n",
    "error=False # Predict errors?\n",
    "model_type=\"sn\" ## \"sn\" or \"camels\" for now\n",
    "# hyperparameters\n",
    "wd         = 0.0005  #value of weight decay\n",
    "dr         = 0.2     #dropout value for fully connected layers\n",
    "hidden     = 5       #this determines the number of channels in the CNNs; integer larger than 1\n",
    "\n",
    "seed       = 123     #random seed to split maps among training, validation and testing\n",
    "splits     = 15      #number of maps per simulation\n",
    "\n",
    "config = {\"learning rate\": lr,\n",
    "                 \"epochs\": epochs,\n",
    "                 \"batch size\": batch_size,\n",
    "                 \"network\": model_type,\n",
    "                 \"dropout\": dr,\n",
    "                 \"error\": error,\n",
    "                 \"hidden\": hidden,\n",
    "                 \"wd\": wd,\n",
    "                 \"splits\":splits}\n",
    "\n",
    "## Initialise wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"%s\" % project_name, entity=\"chris-pedersen\",config=config)\n",
    "\n",
    "## Check if CUDA available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda=True\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda=False\n",
    "\n",
    "## Set number of classes for scattering network to output\n",
    "if error==True:\n",
    "    sn_classes=12\n",
    "else:\n",
    "    sn_classes=6\n",
    "    \n",
    "    \n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n",
    "\n",
    "############################## Set up training params #################################################\n",
    "## camels path\n",
    "camels_path=\"/mnt/ceph/users/camels/PUBLIC_RELEASE/CMD/2D_maps/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0b93a-aca3-4601-91a5-75c4f027b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_B.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_HI.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mcdm.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mgas.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_MgFe.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mstar.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mtot.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_ne.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_P.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_T.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Vcdm.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Vgas.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Z.npy\"\n",
    "\n",
    "# data parameters\n",
    "fmaps      = [\"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_B.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_HI.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_Mgas.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_MgFe.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_Mstar.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_Mtot.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_ne.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_P.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_T.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_Vgas.npy\",\n",
    "              \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_Z.npy\"              \n",
    "             ]\n",
    "fmaps_norm = [None] #if you want to normalize the maps according to the properties of some data set, put that data set here (This is mostly used when training on IllustrisTNG and testing on SIMBA, or vicerversa)\n",
    "fparams    = camels_path+\"/params_IllustrisTNG.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df9dae-56ee-41cc-904b-4995c97b93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "\n",
    "# get training set\n",
    "print('\\nPreparing training set')\n",
    "train_loader = create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=rot_flip_in_mem, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = create_dataset_multifield('valid', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=rot_flip_in_mem,  verbose=True)    \n",
    "\n",
    "# get test set\n",
    "print('\\nPreparing test set')\n",
    "test_loader = create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         rot_flip_in_mem=rot_flip_in_mem,  verbose=True)\n",
    "\n",
    "num_train_maps=len(train_loader.dataset.x)\n",
    "wandb.config.update({\"no. training maps\": num_train_maps})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60c053-c15c-4679-8b63-b2632b12cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type==\"sn\":\n",
    "    ## First create a scattering network object\n",
    "    scatteringBase = baseModelFactory( #creat scattering base model\n",
    "        architecture='scattering',\n",
    "        J=2,\n",
    "        N=256,\n",
    "        M=256,\n",
    "        channels=channels,\n",
    "        max_order=2,\n",
    "        initialization=\"Tight-Frame\",\n",
    "        seed=234,\n",
    "        learnable=True,\n",
    "        lr_orientation=0.005,\n",
    "        lr_scattering=0.005,\n",
    "        skip=True,\n",
    "        split_filters=True,\n",
    "        filter_video=False,\n",
    "        subsample=4,\n",
    "        device=device,\n",
    "        use_cuda=use_cuda\n",
    "    )\n",
    "\n",
    "    ## Now create a network to follow the scattering layers\n",
    "    ## can be MLP, linear, or cnn at the moment\n",
    "    ## (as in https://github.com/bentherien/ParametricScatteringNetworks/ )\n",
    "    top = topModelFactory( #create cnn, mlp, linearlayer, or other\n",
    "        base=scatteringBase,\n",
    "        architecture=\"linear_layer\",\n",
    "        num_classes=sn_classes,\n",
    "        width=5,\n",
    "        average=True,\n",
    "        use_cuda=use_cuda\n",
    "    )\n",
    "\n",
    "    ## Merge these into a hybrid model\n",
    "    hybridModel = sn_HybridModel(scatteringBase=scatteringBase, top=top, use_cuda=use_cuda)\n",
    "    model=hybridModel\n",
    "    wandb.config.update({\"learnable\":scatteringBase.learnable,\n",
    "                         \"initialisation\":scatteringBase.initialization,\n",
    "                         \"wavelet seed\":scatteringBase.seed,\n",
    "                         \"learnable_parameters\":model.countLearnableParams(),\n",
    "                         \"max_order\":scatteringBase.max_order,\n",
    "                         \"skip\":scatteringBase.skip,\n",
    "                         \"split_filters\":scatteringBase.split_filters,\n",
    "                         \"subsample\":scatteringBase.subsample,\n",
    "                         \"scattering_output_dims\":scatteringBase.M_coefficient,\n",
    "                         \"n_coefficients\":scatteringBase.n_coefficients,\n",
    "                         \"top_model\":top.arch,\n",
    "                         \"spatial_average\":top.average\n",
    "                         })\n",
    "    print(\"scattering layer + cnn set up\")\n",
    "else:\n",
    "    print(\"setting up model %s\" % model_type)\n",
    "    model = get_architecture(model_type,hidden,dr,channels)\n",
    "    wandb.config.update({\"learnable_parameters\":sum(p.numel() for p in model.parameters())})\n",
    "model.to(device=device)\n",
    "\n",
    "# wandb\n",
    "wandb.watch(model, log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b33c87-2448-4a9a-a607-33bb105a9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "\n",
    "# do a loop over all epochs\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    log_dic={}\n",
    "    if model_type==\"sn\":\n",
    "        wave_params=hybridModel.scatteringBase.params_filters\n",
    "        orientations=wave_params[0].cpu().detach().numpy()\n",
    "        xis=wave_params[1].cpu().detach().numpy()\n",
    "        sigmas=wave_params[2].cpu().detach().numpy()\n",
    "        slants=wave_params[3].cpu().detach().numpy()\n",
    "        for aa in range(len(orientations)):\n",
    "            log_dic[\"orientation_%d\" % aa]=orientations[aa]\n",
    "            log_dic[\"xi_%d\" % aa]=xis[aa]\n",
    "            log_dic[\"sigma_%d\" % aa]=sigmas[aa]\n",
    "            log_dic[\"slant_%d\" % aa]=slants[aa]\n",
    "\n",
    "    # do training\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]         #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if error==True:\n",
    "            e_NN = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            train_loss2 += loss2*bs\n",
    "        else:\n",
    "            loss = torch.mean(torch.log(loss1))\n",
    "        train_loss1 += loss1*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.log(train_loss1/points) \n",
    "    if error==True:\n",
    "        train_loss+=torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if error==True:    \n",
    "                e_NN  = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                valid_loss2 += loss2*bs\n",
    "            valid_loss1 += loss1*bs\n",
    "            points     += bs\n",
    "\n",
    "    \n",
    "    valid_loss = torch.log(valid_loss1/points) \n",
    "    if error==True:\n",
    "        valid_loss+=torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    log_dic[\"training_loss\"]=train_loss\n",
    "    log_dic[\"valid_loss\"]=valid_loss\n",
    "    wandb.log(log_dic)\n",
    "            \n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "    print(\"\")\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65543a9-e34b-4ca7-bc88-3c7ad1926bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model performance metrics on test set\n",
    "num_maps=test_loader.dataset.size\n",
    "## Now loop over test set and print accuracy\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "# get test loss\n",
    "g = [0, 1, 2, 3, 4, 5]\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        bs    = x.shape[0]    #batch size\n",
    "        x     = x.to(device)  #send data to device\n",
    "        y     = y.to(device)  #send data to device\n",
    "        p     = model(x)      #prediction for mean and variance\n",
    "        y_NN  = p[:,:6]       #prediction for mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if error==True:\n",
    "            e_NN  = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            test_loss2 += loss2*bs\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss = torch.log(test_loss1/points)\n",
    "        if error==True:\n",
    "            test_loss+=torch.log(test_loss2/points)\n",
    "        test_loss = torch.mean(test_loss).item()\n",
    "\n",
    "        #e_NN  = p[:,6:]       #prediction for error\n",
    "        #loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "        #loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "        #test_loss1 += loss1*bs\n",
    "        #test_loss2 += loss2*bs\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        if error==True:\n",
    "            errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "# de-normalize\n",
    "## I guess these are the hardcoded parameter limits\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "\n",
    "\n",
    "test_error = 100*np.mean(np.sqrt((params_true - params_NN)**2)/params_true,axis=0)\n",
    "print('Error Omega_m = %.3f'%test_error[0])\n",
    "print('Error sigma_8 = %.3f'%test_error[1])\n",
    "print('Error A_SN1   = %.3f'%test_error[2])\n",
    "print('Error A_AGN1  = %.3f'%test_error[3])\n",
    "print('Error A_SN2   = %.3f'%test_error[4])\n",
    "print('Error A_AGN2  = %.3f\\n'%test_error[5])\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "wandb.run.summary[\"Error A_SN1\"]  =test_error[2]\n",
    "wandb.run.summary[\"Error A_AGN1\"] =test_error[3]\n",
    "wandb.run.summary[\"Error A_SN2\"]  =test_error[4]\n",
    "wandb.run.summary[\"Error A_AGN2\"] =test_error[5]\n",
    "\n",
    "if error:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "    print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "    print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "    print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "    wandb.run.summary[\"Predicted error A_SN1\"]  =mean_error[2]\n",
    "    wandb.run.summary[\"Predicted error A_AGN1\"] =mean_error[3]\n",
    "    wandb.run.summary[\"Predicted error A_SN2\"]  =mean_error[4]\n",
    "    wandb.run.summary[\"Predicted error A_AGN2\"] =mean_error[5]\n",
    "\n",
    "f, axarr = plt.subplots(3, 2, figsize=(14,20))\n",
    "for aa in range(0,6,2):\n",
    "    axarr[aa//2][0].plot(np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),color=\"black\")\n",
    "    axarr[aa//2][1].plot(np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),color=\"black\")\n",
    "    if error==True:\n",
    "        axarr[aa//2][0].errorbar(params_true[:,aa],params_NN[:,aa],errors_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "        axarr[aa//2][1].errorbar(params_true[:,aa+1],params_NN[:,aa+1],errors_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "    else:\n",
    "        axarr[aa//2][0].plot(params_true[:,aa],params_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "        axarr[aa//2][1].plot(params_true[:,aa+1],params_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "    \n",
    "axarr[0][0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "axarr[0][0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "axarr[0][0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0][0].transAxes)\n",
    "\n",
    "axarr[0][1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "axarr[0][1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "axarr[0][1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[0][1].transAxes)\n",
    "\n",
    "axarr[1][0].set_xlabel(r\"True $A_\\mathrm{SN1}$\")\n",
    "axarr[1][0].set_ylabel(r\"Predicted $A_\\mathrm{SN1}$\")\n",
    "axarr[1][0].text(0.1,0.9,\"%.3f %% error\" % test_error[2],fontsize=12,transform=axarr[1][0].transAxes)\n",
    "\n",
    "axarr[1][1].set_xlabel(r\"True $A_\\mathrm{AGN1}$\")\n",
    "axarr[1][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN1}$\")\n",
    "axarr[1][1].text(0.1,0.9,\"%.3f %% error\" % test_error[3],fontsize=12,transform=axarr[1][1].transAxes)\n",
    "\n",
    "axarr[2][0].set_xlabel(r\"True $A_\\mathrm{SN2}$\")\n",
    "axarr[2][0].set_ylabel(r\"Predicted $A_\\mathrm{SN2}$\")\n",
    "axarr[2][0].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][0].transAxes)\n",
    "\n",
    "axarr[2][1].set_xlabel(r\"True $A_\\mathrm{AGN2}$\")\n",
    "axarr[2][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN2}$\")\n",
    "axarr[2][1].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][1].transAxes)\n",
    "\n",
    "figure=wandb.Image(f)\n",
    "wandb.log({\"performance\": figure})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7cb80-92eb-4d42-84fa-ebc0ac50047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()[\"top.fc1.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdbcbc-1715-451b-8134-6ba300ce1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatteringBase.psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e804ade-d5cb-4574-b918-d6221ab1f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=model.state_dict()[\"top.fc1.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e9d52-329e-46ce-8a66-0cadccfb0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,9))\n",
    "plt.imshow(np.abs(weights.cpu().detach().numpy()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7a038-42fc-4965-bc20-163273bea9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Does this plot depend on initialisation\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.bar(np.arange(73),weights[0].cpu().detach().numpy(),alpha=0.5,label=r\"$\\Omega_m$\")\n",
    "plt.bar(np.arange(73),weights[1].cpu().detach().numpy(),alpha=0.5,label=\"$\\sigma_8$\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.xlabel(\"Field number\")\n",
    "plt.axvline(0.5,color=\"black\")\n",
    "plt.axvline(8.5,color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7801b-aed0-4869-a3f6-b36ef4f88b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop over all maps. For each map, want a 73 length array of the scattering coeffs, and a 6 length array of the params\n",
    "train_loader.get_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd74d2-d921-458d-a533-8030243342b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9888a4-1026-46ae-86da-751e081134de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db4f8c-cc66-44da-8042-71eb5408c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.empty((2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb770f-ba09-4feb-a70e-028e1f427109",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b88389-0f15-4021-8cdd-5f8022458f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "scattered_fields=np.empty((train_loader.dataset.x.shape[0],73))\n",
    "y_values=np.empty((train_loader.dataset.x.shape[0],6))\n",
    "predictions=np.empty((train_loader.dataset.x.shape[0],6))\n",
    "\n",
    "scattered_fields_valid=np.empty((valid_loader.dataset.x.shape[0],73))\n",
    "y_values_valid=np.empty((valid_loader.dataset.x.shape[0],6))\n",
    "predictions_valid=np.empty((valid_loader.dataset.x.shape[0],6))\n",
    "\n",
    "idx=0\n",
    "## first do training set\n",
    "for x,y in train_loader:\n",
    "    bs   = x.shape[0]         #batch size\n",
    "    x    = x.to(device)       #maps\n",
    "    y    = y.to(device)[:,g]  #parameters\n",
    "    p    = model(x)           #NN output\n",
    "    scat = model.scatteringBase.forward(x)\n",
    "    scat = torch.mean(scat, (2,3))\n",
    "    for aa in range(len(scat)):\n",
    "        scattered_field=scat[aa].cpu().detach().numpy()\n",
    "        y_value=y[aa].cpu().detach().numpy()\n",
    "        prediction=p[aa].cpu().detach().numpy()\n",
    "        # Add to output array\n",
    "        scattered_fields[idx]=scattered_field\n",
    "        y_values[idx]=y_value\n",
    "        predictions[idx]=prediction\n",
    "        idx+=1\n",
    "\n",
    "idx=0\n",
    "for x,y in valid_loader:\n",
    "    bs   = x.shape[0]         #batch size\n",
    "    x    = x.to(device)       #maps\n",
    "    y    = y.to(device)[:,g]  #parameters\n",
    "    p    = model(x)           #NN output\n",
    "    scat = model.scatteringBase.forward(x)\n",
    "    scat = torch.mean(scat, (2,3))\n",
    "    for aa in range(len(scat)):\n",
    "        scattered_field=scat[aa].cpu().detach().numpy()\n",
    "        y_value=y[aa].cpu().detach().numpy()\n",
    "        prediction=p[aa].cpu().detach().numpy()\n",
    "        # Add to output array\n",
    "        scattered_fields_valid[idx]=scattered_field\n",
    "        y_values_valid[idx]=y_value\n",
    "        predictions_valid[idx]=prediction\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a69019-fa70-45d6-b17b-62914aa03a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train=[scattered_fields,y_values,predictions]\n",
    "out_valid=[scattered_fields_valid,y_values_valid,predictions_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f3758-dd03-4ec4-986f-26ad6f5ce79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('training_test.p', 'wb') as handle:\n",
    "    pickle.dump(out_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('valid_test.p', 'wb') as handle:\n",
    "    pickle.dump(out_valid, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747538a0-db06-470b-8091-4f939a5c59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f5103-5381-4fcc-a5c7-5a7d9a89aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "scattered_fields[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32471dd-7fa2-4d0f-aa5a-1f059c21b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aa in range(len(scat)):\n",
    "    scattered_field=scat[aa]\n",
    "    print(scattered_field.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c5aa2-68e9-4758-976d-f9b42c712f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5c897-5266-420a-8f67-8d0f9d271941",
   "metadata": {},
   "outputs": [],
   "source": [
    "scat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73446744-a1d6-469e-97e3-ee99a18b4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "scat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c33123-78ed-4b32-b7ae-a8e090a6591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(scat, (2,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ea752-75c6-43a9-a7a5-cd10187007db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775aa4fd-2a87-443c-9271-10b710e7d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046ac7a-5eec-4f3e-9a3f-202b6d58729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.scatteringBase.forward(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24475291-694d-474e-b0e3-305b85e1ae8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet",
   "language": "python",
   "name": "wavelet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
