{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun CAMELs analysis\n",
    "\n",
    "From https://colab.research.google.com/drive/1-BmkA8JSc36O8g9pj7FenD1YSLKqjQR3?usp=sharing#scrollTo=Mo2BXcp0SdlU\n",
    "\n",
    "Going to work through this cell by cell to understand the architecture and see where we need to replace the filters with kymatio transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time, sys, os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Not Available\n"
     ]
    }
   ],
   "source": [
    "## CPU only cuz i have a poopy laptop\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "cudnn.benchmark = True      #May train faster but cost more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "fmaps      = ['maps_Mcdm.npy'] #tuple containing the maps with the different fields to consider\n",
    "fmaps_norm = [None] #if you want to normalize the maps according to the properties of some data set, put that data set here (This is mostly used when training on IllustrisTNG and testing on SIMBA, or vicerversa)\n",
    "fparams    = \"/media/chris/Hard/Sims/CAMELs/params_IllustrisTNG.txt\"\n",
    "seed       = 1   #random seed to split maps among training, validation and testing\n",
    "splits     = 6   #number of maps per simulation\n",
    "\n",
    "# training parameters\n",
    "channels        = 1                #we only consider here 1 field\n",
    "params          = [0,1,2,3,4,5]    #0(Omega_m) 1(sigma_8) 2(A_SN1) 3 (A_AGN1) 4(A_SN2) 5(A_AGN2). The code will be trained to predict all these parameters.\n",
    "g               = params           #g will contain the mean of the posterior\n",
    "h               = [6+i for i in g] #h will contain the variance of the posterior\n",
    "rot_flip_in_mem = False            #whether rotations and flipings are kept in memory. True will make the code faster but consumes more RAM memory.\n",
    "\n",
    "# optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 128\n",
    "lr         = 1e-3\n",
    "wd         = 0.0005  #value of weight decay\n",
    "dr         = 0.2    #dropout value for fully connected layers\n",
    "hidden     = 5      #this determines the number of channels in the CNNs; integer larger than 1\n",
    "epochs     = 10    #number of epochs to train the network\n",
    "\n",
    "# output files names\n",
    "floss  = 'loss.txt'   #file with the training and validation losses for each epoch\n",
    "fmodel = 'weights.pt' #file containing the weights of the best-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the maps: (15000, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "fmaps2 = \"/media/chris/Hard/Sims/CAMELs/Maps_Mcdm_IllustrisTNG_LH_z=0.00.npy\"\n",
    "maps  = np.load(fmaps2)\n",
    "print('Shape of the maps:',maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6000 maps out of 15000\n"
     ]
    }
   ],
   "source": [
    "# define the array that will contain the indexes of the maps\n",
    "indexes = np.zeros(1000*splits, dtype=np.int32)\n",
    "\n",
    "# do a loop over all maps and choose the ones we want\n",
    "count = 0\n",
    "for i in range(15000):\n",
    "    if i%15 in np.arange(splits):  \n",
    "      indexes[count] = i\n",
    "      count += 1\n",
    "print('Selected %d maps out of 15000'%count)\n",
    "\n",
    "# save these maps to a new file\n",
    "maps = maps[indexes]\n",
    "np.save('maps_Mcdm.npy', maps)\n",
    "del maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "2.909e+09 < F(all|orig) < 2.789e+15\n",
      "9.464 < F(all|resc)  < 15.445\n",
      "-2.993 < F(all|norm) < 8.789\n",
      "\n",
      "Preparing validation set\n",
      "Found 1 channels\n",
      "Reading data...\n",
      "2.909e+09 < F(all|orig) < 2.789e+15\n",
      "9.464 < F(all|resc)  < 15.445\n",
      "-2.993 < F(all|norm) < 8.789\n",
      "Channel 0 contains 2400 maps\n",
      "-2.539 < F < 8.196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This routine returns the data loader need to train the network\n",
    "def create_dataset_multifield(mode, seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                              rot_flip_in_mem=True, shuffle=True, verbose=False):\n",
    "\n",
    "    # whether rotations and flippings are kept in memory\n",
    "    if rot_flip_in_mem:\n",
    "        data_set = make_dataset_multifield(mode, seed, fmaps, fparams, splits, fmaps_norm, verbose)\n",
    "    else:\n",
    "        data_set = make_dataset_multifield2(mode, seed, fmaps, fparams, splits, fmaps_norm, verbose)\n",
    "\n",
    "    data_loader = DataLoader(dataset=data_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# This class creates the dataset. Rotations and flippings are precompued and stored in memory\n",
    "class make_dataset_multifield():\n",
    "\n",
    "    def __init__(self, mode, seed, fmaps, fparams, splits, fmaps_norm, verbose):\n",
    "\n",
    "        # get the total number of sims and maps\n",
    "        params_sims = np.loadtxt(fparams) #simulations parameters, NOT maps parameters\n",
    "        total_sims, total_maps, num_params = \\\n",
    "                params_sims.shape[0], params_sims.shape[0]*splits, params_sims.shape[1]\n",
    "        params_maps = np.zeros((total_maps, num_params), dtype=np.float32)\n",
    "        for i in range(total_sims):\n",
    "            for j in range(splits):\n",
    "                params_maps[i*splits + j] = params_sims[i]\n",
    "\n",
    "        # normalize the value of the cosmological & astrophysical parameters\n",
    "        minimum     = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "        maximum     = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "        params_maps = (params_maps - minimum)/(maximum - minimum)\n",
    "\n",
    "        # get the size and offset depending on the type of dataset\n",
    "        if   mode=='train':  offset, size_sims = int(0.00*total_sims), int(0.90*total_sims)\n",
    "        elif mode=='valid':  offset, size_sims = int(0.90*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='test':   offset, size_sims = int(0.95*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='all':    offset, size_sims = int(0.00*total_sims), int(1.00*total_sims)\n",
    "        else:                raise Exception('Wrong name!')\n",
    "        size_maps = size_sims*splits\n",
    "\n",
    "        # randomly shuffle the simulations (not maps). Instead of 0 1 2 3...999 have a \n",
    "        # random permutation. E.g. 5 9 0 29...342\n",
    "        np.random.seed(seed)\n",
    "        sim_numbers = np.arange(total_sims) #shuffle sims not maps\n",
    "        np.random.shuffle(sim_numbers)\n",
    "        sim_numbers = sim_numbers[offset:offset+size_sims] #select indexes of mode\n",
    "\n",
    "        # get the corresponding indexes of the maps associated to the sims\n",
    "        indexes = np.zeros(size_maps, dtype=np.int32)\n",
    "        count = 0\n",
    "        for i in sim_numbers:\n",
    "            for j in range(splits):\n",
    "                indexes[count] = i*splits + j\n",
    "                count += 1\n",
    "\n",
    "        # keep only the value of the parameters of the considered maps\n",
    "        params_maps = params_maps[indexes]\n",
    "\n",
    "        # define the matrix containing the maps with rotations and flipings\n",
    "        channels = len(fmaps)\n",
    "        dumb     = np.load(fmaps[0])    #[number of maps, height, width]\n",
    "        height, width = dumb.shape[1], dumb.shape[2];  del dumb\n",
    "        data     = np.zeros((size_maps*8, channels, height, width), dtype=np.float32)\n",
    "        params   = np.zeros((size_maps*8, num_params),              dtype=np.float32)\n",
    "\n",
    "        # read the data\n",
    "        print('Found %d channels\\nReading data...'%channels)\n",
    "        for channel, (fim, fnorm) in enumerate(zip(fmaps, fmaps_norm)):\n",
    "\n",
    "            # read maps in the considered channel\n",
    "            data_c = np.load(fim)\n",
    "            if data_c.shape[0]!=total_maps:  raise Exception('sizes do not match')\n",
    "            if verbose:  print('%.3e < F(all|orig) < %.3e'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "            # rescale maps\n",
    "            if fim.find('Mstar')!=-1:  data_c = np.log10(data_c + 1.0)\n",
    "            else:                      data_c = np.log10(data_c)\n",
    "            if verbose:  print('%.3f < F(all|resc)  < %.3f'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "            # normalize maps\n",
    "            if fnorm is None:  \n",
    "                mean,    std     = np.mean(data_c), np.std(data_c)\n",
    "            else:\n",
    "                # read data\n",
    "                data_norm = np.load(fnorm)\n",
    "\n",
    "                # rescale\n",
    "                if fnorm.find('Mstar')!=-1:  data_norm = np.log10(data_norm + 1.0)\n",
    "                else:                        data_norm = np.log10(data_norm)\n",
    "\n",
    "                # compute mean and std\n",
    "                mean,    std     = np.mean(data_norm), np.std(data_norm)\n",
    "                del data_norm\n",
    "\n",
    "            data_c = (data_c - mean)/std\n",
    "            if verbose:  print('%.3f < F(all|norm) < %.3f'%(np.min(data_c), np.max(data_c))) \n",
    "\n",
    "            # keep only the data of the chosen set\n",
    "            data_c = data_c[indexes]\n",
    "\n",
    "            # do a loop over all rotations (each is 90 deg)\n",
    "            counted_maps = 0\n",
    "            for rot in [0,1,2,3]:\n",
    "                data_rot = np.rot90(data_c, k=rot, axes=(1,2))\n",
    "\n",
    "                data[counted_maps:counted_maps+size_maps,channel,:,:] = data_rot\n",
    "                params[counted_maps:counted_maps+size_maps]           = params_maps\n",
    "                counted_maps += size_maps\n",
    "\n",
    "                data[counted_maps:counted_maps+size_maps,channel,:,:] = \\\n",
    "                                                    np.flip(data_rot, axis=1)\n",
    "                params[counted_maps:counted_maps+size_maps]           = params_maps\n",
    "                counted_maps += size_maps\n",
    "            \n",
    "            if verbose:\n",
    "                print('Channel %d contains %d maps'%(channel,counted_maps))\n",
    "                print('%.3f < F < %.3f\\n'%(np.min(data_c), np.max(data_c)))\n",
    "                \n",
    "        self.size = data.shape[0]\n",
    "        self.x    = torch.tensor(data,   dtype=torch.float32)\n",
    "        self.y    = torch.tensor(params, dtype=torch.float32)\n",
    "        del data, data_c\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# This class creates the dataset. It will read the maps and store them in memory.\n",
    "# The rotations and flipings are done when calling the data \n",
    "class make_dataset_multifield2():\n",
    "\n",
    "    def __init__(self, mode, seed, fmaps, fparams, splits, fmaps_norm, verbose):\n",
    "\n",
    "        # get the total number of simulations and maps\n",
    "        params_sims = np.loadtxt(fparams) #simulations parameters, NOT maps parameters\n",
    "        total_sims, total_maps, num_params = \\\n",
    "                params_sims.shape[0], params_sims.shape[0]*splits, params_sims.shape[1]\n",
    "        params = np.zeros((total_maps, num_params), dtype=np.float32)\n",
    "        for i in range(total_sims):\n",
    "            for j in range(splits):\n",
    "                params[i*splits + j] = params_sims[i]\n",
    "\n",
    "        # normalize params\n",
    "        minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "        maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "        params  = (params - minimum)/(maximum - minimum)\n",
    "\n",
    "        # get the size and offset depending on the type of dataset\n",
    "        if   mode=='train':  offset, size_sims = int(0.00*total_sims), int(0.90*total_sims)\n",
    "        elif mode=='valid':  offset, size_sims = int(0.90*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='test':   offset, size_sims = int(0.95*total_sims), int(0.05*total_sims)\n",
    "        elif mode=='all':    offset, size_sims = int(0.00*total_sims), int(1.00*total_sims)\n",
    "        else:                raise Exception('Wrong name!')\n",
    "        size_maps = size_sims*splits\n",
    "\n",
    "        # randomly shuffle the simulations (not maps). Instead of 0 1 2 3...999 have a \n",
    "        # random permutation. E.g. 5 9 0 29...342\n",
    "        np.random.seed(seed)\n",
    "        sim_numbers = np.arange(total_sims) #shuffle maps not rotations\n",
    "        np.random.shuffle(sim_numbers)\n",
    "        sim_numbers = sim_numbers[offset:offset+size_sims] #select indexes of mode\n",
    "\n",
    "        # get the corresponding indexes of the maps associated to the sims\n",
    "        indexes = np.zeros(size_maps, dtype=np.int32)\n",
    "        count = 0\n",
    "        for i in sim_numbers:\n",
    "            for j in range(splits):\n",
    "                indexes[count] = i*splits + j\n",
    "                count += 1\n",
    "\n",
    "        # keep only the value of the parameters of the considered maps\n",
    "        params = params[indexes]\n",
    "\n",
    "        # define the matrix containing the maps without rotations or flippings\n",
    "        channels = len(fmaps)\n",
    "        dumb     = np.load(fmaps[0])    #[number of maps, height, width]\n",
    "        height, width = dumb.shape[1], dumb.shape[2];  del dumb\n",
    "        data     = np.zeros((size_maps, channels, height, width), dtype=np.float32)\n",
    "\n",
    "        # read the data\n",
    "        print('Found %d channels\\nReading data...'%channels)\n",
    "        for channel, (fim, fnorm) in enumerate(zip(fmaps, fmaps_norm)):\n",
    "\n",
    "            # read maps in the considered channel\n",
    "            data_c = np.load(fim)\n",
    "            if data_c.shape[0]!=total_maps:  raise Exception('sizes do not match')\n",
    "            if verbose:  \n",
    "                print('%.3e < F(all|orig) < %.3e'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "            # rescale maps\n",
    "            if fim.find('Mstar')!=-1:  data_c = np.log10(data_c + 1.0)\n",
    "            else:                      data_c = np.log10(data_c)\n",
    "            if verbose:  \n",
    "                print('%.3f < F(all|resc)  < %.3f'%(np.min(data_c), np.max(data_c)))\n",
    "\n",
    "            # normalize maps\n",
    "            if fnorm is None:  \n",
    "                mean,    std     = np.mean(data_c), np.std(data_c)\n",
    "                minimum, maximum = np.min(data_c),  np.max(data_c)\n",
    "            else:\n",
    "                # read data\n",
    "                data_norm     = np.load(fnorm)\n",
    "\n",
    "                # rescale data\n",
    "                if fnorm.find('Mstar')!=-1:  data_norm = np.log10(data_norm + 1.0)\n",
    "                else:                        data_norm = np.log10(data_norm)\n",
    "\n",
    "                # compute mean and std\n",
    "                mean,    std     = np.mean(data_norm), np.std(data_norm)\n",
    "                minimum, maximum = np.min(data_norm),  np.max(data_norm)\n",
    "                del data_norm\n",
    "\n",
    "            data_c = (data_c - mean)/std\n",
    "            if verbose:  print('%.3f < F(all|norm) < %.3f'%(np.min(data_c), np.max(data_c))) \n",
    "\n",
    "            # keep only the data of the chosen set\n",
    "            data[:,channel,:,:] = data_c[indexes]\n",
    "        \n",
    "        self.size = data.shape[0]\n",
    "        self.x    = torch.tensor(data,   dtype=torch.float32)\n",
    "        self.y    = torch.tensor(params, dtype=torch.float32)\n",
    "        del data, data_c\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # choose a rotation angle (0-0, 1-90, 2-180, 3-270)\n",
    "        # and whether do flipping or not\n",
    "        rot  = np.random.randint(0,4)\n",
    "        flip = np.random.randint(0,1)\n",
    "        \n",
    "        # rotate and flip the maps\n",
    "        maps = torch.rot90(self.x[idx], k=rot, dims=[1,2])\n",
    "        if flip==1:  maps = torch.flip(maps, dims=[1])\n",
    "\n",
    "        return maps, self.y[idx]\n",
    "\n",
    "# get training set\n",
    "print('\\nPreparing training set')\n",
    "train_loader = create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=rot_flip_in_mem, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = create_dataset_multifield('valid', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=True,  verbose=True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters in the model = 5881752\n"
     ]
    }
   ],
   "source": [
    "class model_o3_err(nn.Module):\n",
    "    def __init__(self, hidden, dr, channels):\n",
    "        super(model_o3_err, self).__init__()\n",
    "        \n",
    "        # input: 1x256x256 ---------------> output: 2*hiddenx128x128\n",
    "        self.C01 = nn.Conv2d(channels,  2*hidden, kernel_size=3, stride=1, padding=1, \n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C02 = nn.Conv2d(2*hidden,  2*hidden, kernel_size=3, stride=1, padding=1, \n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C03 = nn.Conv2d(2*hidden,  2*hidden, kernel_size=2, stride=2, padding=0, \n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B01 = nn.BatchNorm2d(2*hidden)\n",
    "        self.B02 = nn.BatchNorm2d(2*hidden)\n",
    "        self.B03 = nn.BatchNorm2d(2*hidden)\n",
    "        \n",
    "        # input: 2*hiddenx128x128 ----------> output: 4*hiddenx64x64\n",
    "        self.C11 = nn.Conv2d(2*hidden, 4*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C12 = nn.Conv2d(4*hidden, 4*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C13 = nn.Conv2d(4*hidden, 4*hidden, kernel_size=2, stride=2, padding=0,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B11 = nn.BatchNorm2d(4*hidden)\n",
    "        self.B12 = nn.BatchNorm2d(4*hidden)\n",
    "        self.B13 = nn.BatchNorm2d(4*hidden)\n",
    "        \n",
    "        # input: 4*hiddenx64x64 --------> output: 8*hiddenx32x32\n",
    "        self.C21 = nn.Conv2d(4*hidden, 8*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C22 = nn.Conv2d(8*hidden, 8*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C23 = nn.Conv2d(8*hidden, 8*hidden, kernel_size=2, stride=2, padding=0,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B21 = nn.BatchNorm2d(8*hidden)\n",
    "        self.B22 = nn.BatchNorm2d(8*hidden)\n",
    "        self.B23 = nn.BatchNorm2d(8*hidden)\n",
    "        \n",
    "        # input: 8*hiddenx32x32 ----------> output: 16*hiddenx16x16\n",
    "        self.C31 = nn.Conv2d(8*hidden,  16*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C32 = nn.Conv2d(16*hidden, 16*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C33 = nn.Conv2d(16*hidden, 16*hidden, kernel_size=2, stride=2, padding=0,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B31 = nn.BatchNorm2d(16*hidden)\n",
    "        self.B32 = nn.BatchNorm2d(16*hidden)\n",
    "        self.B33 = nn.BatchNorm2d(16*hidden)\n",
    "        \n",
    "        # input: 16*hiddenx16x16 ----------> output: 32*hiddenx8x8\n",
    "        self.C41 = nn.Conv2d(16*hidden, 32*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C42 = nn.Conv2d(32*hidden, 32*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C43 = nn.Conv2d(32*hidden, 32*hidden, kernel_size=2, stride=2, padding=0,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B41 = nn.BatchNorm2d(32*hidden)\n",
    "        self.B42 = nn.BatchNorm2d(32*hidden)\n",
    "        self.B43 = nn.BatchNorm2d(32*hidden)\n",
    "        \n",
    "        # input: 32*hiddenx8x8 ----------> output:64*hiddenx4x4\n",
    "        self.C51 = nn.Conv2d(32*hidden, 64*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C52 = nn.Conv2d(64*hidden, 64*hidden, kernel_size=3, stride=1, padding=1,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.C53 = nn.Conv2d(64*hidden, 64*hidden, kernel_size=2, stride=2, padding=0,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B51 = nn.BatchNorm2d(64*hidden)\n",
    "        self.B52 = nn.BatchNorm2d(64*hidden)\n",
    "        self.B53 = nn.BatchNorm2d(64*hidden)\n",
    "\n",
    "        # input: 64*hiddenx4x4 ----------> output: 128*hiddenx1x1\n",
    "        self.C61 = nn.Conv2d(64*hidden, 128*hidden, kernel_size=4, stride=1, padding=0,\n",
    "                            padding_mode='circular', bias=True)\n",
    "        self.B61 = nn.BatchNorm2d(128*hidden)\n",
    "\n",
    "        self.P0  = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.FC1  = nn.Linear(128*hidden, 64*hidden)  \n",
    "        self.FC2  = nn.Linear(64*hidden,  12)  \n",
    "\n",
    "        self.dropout   = nn.Dropout(p=dr)\n",
    "        self.ReLU      = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.tanh      = nn.Tanh()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.LeakyReLU(self.C01(image))\n",
    "        x = self.LeakyReLU(self.B02(self.C02(x)))\n",
    "        x = self.LeakyReLU(self.B03(self.C03(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B11(self.C11(x)))\n",
    "        x = self.LeakyReLU(self.B12(self.C12(x)))\n",
    "        x = self.LeakyReLU(self.B13(self.C13(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B21(self.C21(x)))\n",
    "        x = self.LeakyReLU(self.B22(self.C22(x)))\n",
    "        x = self.LeakyReLU(self.B23(self.C23(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B31(self.C31(x)))\n",
    "        x = self.LeakyReLU(self.B32(self.C32(x)))\n",
    "        x = self.LeakyReLU(self.B33(self.C33(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B41(self.C41(x)))\n",
    "        x = self.LeakyReLU(self.B42(self.C42(x)))\n",
    "        x = self.LeakyReLU(self.B43(self.C43(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B51(self.C51(x)))\n",
    "        x = self.LeakyReLU(self.B52(self.C52(x)))\n",
    "        x = self.LeakyReLU(self.B53(self.C53(x)))\n",
    "\n",
    "        x = self.LeakyReLU(self.B61(self.C61(x)))\n",
    "\n",
    "        x = x.view(image.shape[0],-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(self.LeakyReLU(self.FC1(x)))\n",
    "        x = self.FC2(x)\n",
    "\n",
    "        # enforce the errors to be positive\n",
    "        y = torch.clone(x)\n",
    "        y[:,6:12] = torch.square(x[:,6:12])\n",
    "\n",
    "        return y\n",
    "\n",
    "# now that architecture is defined above, use it\n",
    "model = model_o3_err(hidden, dr, channels)\n",
    "model.to(device=device)\n",
    "network_total_params = sum(p.numel() for p in model.parameters())\n",
    "print('total number of parameters in the model = %d'%network_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing initial validation loss\n",
      "Initial valid loss = 1.066e+01\n"
     ]
    }
   ],
   "source": [
    "print('Computing initial validation loss')\n",
    "model.eval()\n",
    "valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "min_valid_loss, points = 0.0, 0\n",
    "for x, y in valid_loader:\n",
    "      with torch.no_grad():\n",
    "          bs   = x.shape[0]                #batch size\n",
    "          x    = x.to(device=device)       #maps\n",
    "          y    = y.to(device=device)[:,g]  #parameters\n",
    "          p    = model(x)                  #NN output\n",
    "          y_NN = p[:,g]                    #posterior mean\n",
    "          e_NN = p[:,h]                    #posterior std\n",
    "          loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "          loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "          loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "          valid_loss1 += loss1*bs\n",
    "          valid_loss2 += loss2*bs\n",
    "          points += bs\n",
    "min_valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "min_valid_loss = torch.mean(min_valid_loss).item()\n",
    "print('Initial valid loss = %.3e'%min_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a loop over all epochs\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # do training\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]         #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        e_NN = p[:,h]             #posterior std\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        train_loss1 += loss1*bs\n",
    "        train_loss2 += loss2*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if points>18000:  break\n",
    "    train_loss = torch.log(train_loss1/points) + torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            e_NN  = p[:,h]             #posterior std\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            valid_loss1 += loss1*bs\n",
    "            valid_loss2 += loss2*bs\n",
    "            points     += bs\n",
    "    valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "\n",
    "    # save model if it is better\n",
    "    if valid_loss<min_valid_loss:\n",
    "        torch.save(model.state_dict(), fmodel)\n",
    "        min_valid_loss = valid_loss\n",
    "        print('(C) ', end='')\n",
    "    print('')\n",
    "\n",
    "    # save losses to file\n",
    "    f = open(floss, 'a')\n",
    "    f.write('%d %.5e %.5e\\n'%(epoch, train_loss, valid_loss))\n",
    "    f.close()\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights in case they exists\n",
    "if os.path.exists(fmodel):  \n",
    "    model.load_state_dict(torch.load(fmodel, map_location=torch.device(device)))\n",
    "    print('Weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test_loader  = create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=False, verbose=True)\n",
    "\n",
    "# get the number of maps in the test set\n",
    "num_maps = 0\n",
    "for x,y in test_loader:\n",
    "      num_maps += x.shape[0]\n",
    "print('\\nNumber of maps in the test set: %d'%num_maps)\n",
    "\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "# get test loss\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        bs    = x.shape[0]    #batch size\n",
    "        x     = x.to(device)  #send data to device\n",
    "        y     = y.to(device)  #send data to device\n",
    "        p     = model(x)      #prediction for mean and variance\n",
    "        y_NN  = p[:,:6]       #prediction for mean\n",
    "        e_NN  = p[:,6:]       #prediction for error\n",
    "        loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "        loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss2 += loss2*bs\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "Norm_error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Normalized Error Omega_m = %.3f'%Norm_error[0])\n",
    "print('Normalized Error sigma_8 = %.3f'%Norm_error[1])\n",
    "print('Normalized Error A_SN1   = %.3f'%Norm_error[2])\n",
    "print('Normalized Error A_AGN1  = %.3f'%Norm_error[3])\n",
    "print('Normalized Error A_SN2   = %.3f'%Norm_error[4])\n",
    "print('Normalized Error A_AGN2  = %.3f\\n'%Norm_error[5])\n",
    "\n",
    "# de-normalize\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "errors_NN   = errors_NN*(maximum - minimum)\n",
    "\n",
    "error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Error Omega_m = %.3f'%error[0])\n",
    "print('Error sigma_8 = %.3f'%error[1])\n",
    "print('Error A_SN1   = %.3f'%error[2])\n",
    "print('Error A_AGN1  = %.3f'%error[3])\n",
    "print('Error A_SN2   = %.3f'%error[4])\n",
    "print('Error A_AGN2  = %.3f\\n'%error[5])\n",
    "\n",
    "mean_error = np.absolute(np.mean(errors_NN, axis=0))\n",
    "print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "\n",
    "rel_error = np.sqrt(np.mean((params_true - params_NN)**2/params_true**2, axis=0))\n",
    "print('Relative error Omega_m = %.3f'%rel_error[0])\n",
    "print('Relative error sigma_8 = %.3f'%rel_error[1])\n",
    "print('Relative error A_SN1   = %.3f'%rel_error[2])\n",
    "print('Relative error A_AGN1  = %.3f'%rel_error[3])\n",
    "print('Relative error A_SN2   = %.3f'%rel_error[4])\n",
    "print('Relative error A_AGN2  = %.3f\\n'%rel_error[5])\n",
    "\n",
    "# save results to file\n",
    "#dataset = np.zeros((num_maps,18), dtype=np.float32)\n",
    "#dataset[:,:6]   = params_true\n",
    "#dataset[:,6:12] = params_NN\n",
    "#dataset[:,12:]  = errors_NN\n",
    "#np.savetxt(fresults,  dataset)\n",
    "#np.savetxt(fresults1, Norm_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first map of every simulation in the test set\n",
    "indexes = np.arange(50)*splits\n",
    "\n",
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.1, 0.45, r'$\\Omega_{\\rm m}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,0], params_NN[indexes,0], errors_NN[indexes,0], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.1,0.5], [0.1,0.5], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.6, 0.95, r'$\\sigma_8$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,1], params_NN[indexes,1], errors_NN[indexes,1], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.6,1.0], [0.6,1.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.25, 4.0, r'$A_{\\rm SN1}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,2], params_NN[indexes,2], errors_NN[indexes,2], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.25,4.0], [0.25,4.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
