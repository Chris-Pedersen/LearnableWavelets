{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun CAMELs analysis - single map\n",
    "\n",
    "From https://colab.research.google.com/drive/1-BmkA8JSc36O8g9pj7FenD1YSLKqjQR3?usp=sharing#scrollTo=Mo2BXcp0SdlU\n",
    "\n",
    "Going to work through this cell by cell to understand the architecture and see where we need to replace the filters with kymatio transforms.\n",
    "\n",
    "Here we attempt to run the network on a single map to see if we can overfit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time, sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# my modules\n",
    "from sn_camels.models.models_factory import baseModelFactory, topModelFactory\n",
    "from sn_camels.models.sn_hybrid_models import sn_HybridModel\n",
    "from sn_camels.models.camels_models import model_o3_err\n",
    "from sn_camels.camels.camels_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatteringBase = baseModelFactory( #creat scattering base model\n",
    "    architecture='scattering',\n",
    "    J=2,\n",
    "    N=256,\n",
    "    M=256,\n",
    "    max_order=2,\n",
    "    initialization=\"Random\",\n",
    "    seed=123,\n",
    "    learnable=False,\n",
    "    lr_orientation=0.1,\n",
    "    lr_scattering=0.1,\n",
    "    skip=True,\n",
    "    split_filters=False,\n",
    "    subsample=4,\n",
    "    filter_video=False,\n",
    "    device=\"cpu\",\n",
    "    use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatteringBase.n_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = topModelFactory( #create cnn, mlp, linearlayer, or other\n",
    "    base=scatteringBase,\n",
    "    architecture=\"linear_layer\",\n",
    "    num_classes=12, \n",
    "    width=8,\n",
    "    average=True,\n",
    "    use_cuda=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scatteringBase.scattering.psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybridModel = sn_HybridModel(scatteringBase=scatteringBase, top=top, use_cuda=False) #creat hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CPU only cuz i have a poopy laptop\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "cudnn.benchmark = True      #May train faster but cost more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## camels path\n",
    "camels_path=os.environ['CAMELS_PATH']\n",
    "\n",
    "# data parameters\n",
    "fmaps      = ['maps_Mcdm.npy'] #tuple containing the maps with the different fields to consider\n",
    "fmaps_norm = [None] #if you want to normalize the maps according to the properties of some data set, put that data set here (This is mostly used when training on IllustrisTNG and testing on SIMBA, or vicerversa)\n",
    "fparams    = camels_path+\"/params_IllustrisTNG.txt\"\n",
    "seed       = 1   #random seed to split maps among training, validation and testing\n",
    "splits     = 1   #number of maps per simulation\n",
    "\n",
    "# training parameters\n",
    "channels        = 1                #we only consider here 1 field\n",
    "params          = [0,1,2,3,4,5]    #0(Omega_m) 1(sigma_8) 2(A_SN1) 3 (A_AGN1) 4(A_SN2) 5(A_AGN2). The code will be trained to predict all these parameters.\n",
    "g               = params           #g will contain the mean of the posterior\n",
    "h               = [6+i for i in g] #h will contain the variance of the posterior\n",
    "rot_flip_in_mem = False            #whether rotations and flipings are kept in memory. True will make the code faster but consumes more RAM memory.\n",
    "\n",
    "# optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 10\n",
    "lr         = 1e-3\n",
    "wd         = 0.0005  #value of weight decay\n",
    "dr         = 0.2    #dropout value for fully connected layers\n",
    "hidden     = 5      #this determines the number of channels in the CNNs; integer larger than 1\n",
    "epochs     = 10    #number of epochs to train the network\n",
    "\n",
    "# output files names\n",
    "floss  = 'loss.txt'   #file with the training and validation losses for each epoch\n",
    "fmodel = 'weights.pt' #file containing the weights of the best-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmaps2 = camels_path+\"/Maps_Mcdm_IllustrisTNG_LH_z=0.00.npy\"\n",
    "maps  = np.load(fmaps2)\n",
    "print('Shape of the maps:',maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the array that will contain the indexes of the maps\n",
    "indexes = np.zeros(1000*splits, dtype=np.int32)\n",
    "\n",
    "# do a loop over all maps and choose the ones we want\n",
    "count = 0\n",
    "for i in range(15000):\n",
    "    if i%15 in np.arange(splits):  \n",
    "      indexes[count] = i\n",
    "      count += 1\n",
    "print('Selected %d maps out of 15000'%count)\n",
    "\n",
    "# save these maps to a new file\n",
    "maps = maps[indexes]\n",
    "np.save('maps_Mcdm.npy', maps)\n",
    "del maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training set\n",
    "print('\\nPreparing training set')\n",
    "train_loader = create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=rot_flip_in_mem, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hybridModel\n",
    "#model2 = model_o3_err(hidden, dr, channels)\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.scatteringBase.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(model.scatteringBase.forward(x), (2,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:,g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:,h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a loop over all epochs for a single batch\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "\n",
    "    # do training\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    bs   = x.shape[0]         #batch size\n",
    "    x    = x.to(device)       #maps\n",
    "    y    = y.to(device)[:,g]  #parameters\n",
    "    p    = model(x)           #NN output\n",
    "    y_NN = p[:,g]             #posterior mean\n",
    "    e_NN = p[:,h]             #posterior std\n",
    "    loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "    loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "    loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "    train_loss1 += loss1*bs\n",
    "    train_loss2 += loss2*bs\n",
    "    points      += bs\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #if points>18000:  break\n",
    "    train_loss = torch.log(train_loss1/points) + torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    #valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    #valid_loss, points = 0.0, 0\n",
    "    print(train_loss)\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a loop over all epochs\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # do training\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]         #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        e_NN = p[:,h]             #posterior std\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        train_loss1 += loss1*bs\n",
    "        train_loss2 += loss2*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if points>18000:  break\n",
    "    train_loss = torch.log(train_loss1/points) + torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            e_NN  = p[:,h]             #posterior std\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            valid_loss1 += loss1*bs\n",
    "            valid_loss2 += loss2*bs\n",
    "            points     += bs\n",
    "    valid_loss = torch.log(valid_loss1/points) + torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "\n",
    "    # save model if it is better\n",
    "    if valid_loss<min_valid_loss:\n",
    "        torch.save(model.state_dict(), fmodel)\n",
    "        min_valid_loss = valid_loss\n",
    "        print('(C) ', end='')\n",
    "    print('')\n",
    "\n",
    "    # save losses to file\n",
    "    f = open(floss, 'a')\n",
    "    f.write('%d %.5e %.5e\\n'%(epoch, train_loss, valid_loss))\n",
    "    f.close()\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights in case they exists\n",
    "if os.path.exists(fmodel):  \n",
    "    model.load_state_dict(torch.load(fmodel, map_location=torch.device(device)))\n",
    "    print('Weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test_loader  = create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=False, verbose=True)\n",
    "\n",
    "# get the number of maps in the test set\n",
    "num_maps = 0\n",
    "for x,y in test_loader:\n",
    "      num_maps += x.shape[0]\n",
    "print('\\nNumber of maps in the test set: %d'%num_maps)\n",
    "\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "# get test loss\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        bs    = x.shape[0]    #batch size\n",
    "        x     = x.to(device)  #send data to device\n",
    "        y     = y.to(device)  #send data to device\n",
    "        p     = model(x)      #prediction for mean and variance\n",
    "        y_NN  = p[:,:6]       #prediction for mean\n",
    "        e_NN  = p[:,6:]       #prediction for error\n",
    "        loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "        loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss2 += loss2*bs\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "Norm_error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Normalized Error Omega_m = %.3f'%Norm_error[0])\n",
    "print('Normalized Error sigma_8 = %.3f'%Norm_error[1])\n",
    "print('Normalized Error A_SN1   = %.3f'%Norm_error[2])\n",
    "print('Normalized Error A_AGN1  = %.3f'%Norm_error[3])\n",
    "print('Normalized Error A_SN2   = %.3f'%Norm_error[4])\n",
    "print('Normalized Error A_AGN2  = %.3f\\n'%Norm_error[5])\n",
    "\n",
    "# de-normalize\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "errors_NN   = errors_NN*(maximum - minimum)\n",
    "\n",
    "error = np.sqrt(np.mean((params_true - params_NN)**2, axis=0))\n",
    "print('Error Omega_m = %.3f'%error[0])\n",
    "print('Error sigma_8 = %.3f'%error[1])\n",
    "print('Error A_SN1   = %.3f'%error[2])\n",
    "print('Error A_AGN1  = %.3f'%error[3])\n",
    "print('Error A_SN2   = %.3f'%error[4])\n",
    "print('Error A_AGN2  = %.3f\\n'%error[5])\n",
    "\n",
    "mean_error = np.absolute(np.mean(errors_NN, axis=0))\n",
    "print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "\n",
    "rel_error = np.sqrt(np.mean((params_true - params_NN)**2/params_true**2, axis=0))\n",
    "print('Relative error Omega_m = %.3f'%rel_error[0])\n",
    "print('Relative error sigma_8 = %.3f'%rel_error[1])\n",
    "print('Relative error A_SN1   = %.3f'%rel_error[2])\n",
    "print('Relative error A_AGN1  = %.3f'%rel_error[3])\n",
    "print('Relative error A_SN2   = %.3f'%rel_error[4])\n",
    "print('Relative error A_AGN2  = %.3f\\n'%rel_error[5])\n",
    "\n",
    "# save results to file\n",
    "#dataset = np.zeros((num_maps,18), dtype=np.float32)\n",
    "#dataset[:,:6]   = params_true\n",
    "#dataset[:,6:12] = params_NN\n",
    "#dataset[:,12:]  = errors_NN\n",
    "#np.savetxt(fresults,  dataset)\n",
    "#np.savetxt(fresults1, Norm_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first map of every simulation in the test set\n",
    "indexes = np.arange(50)*splits\n",
    "\n",
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.1, 0.45, r'$\\Omega_{\\rm m}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,0], params_NN[indexes,0], errors_NN[indexes,0], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.1,0.5], [0.1,0.5], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.6, 0.95, r'$\\sigma_8$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,1], params_NN[indexes,1], errors_NN[indexes,1], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.6,1.0], [0.6,1.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.xlabel(r'${\\rm Truth}$')\n",
    "plt.ylabel(r'${\\rm Inference}$')\n",
    "plt.text(0.25, 4.0, r'$A_{\\rm SN1}$',fontsize=18)\n",
    "\n",
    "plt.errorbar(params_true[indexes,2], params_NN[indexes,2], errors_NN[indexes,2], \n",
    "             linestyle='None', lw=1, fmt='o', ms=2, elinewidth=1, capsize=0, c='r')\n",
    "plt.plot([0.25,4.0], [0.25,4.0], color='k')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
