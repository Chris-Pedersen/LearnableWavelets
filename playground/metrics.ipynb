{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99e57b8-ac02-4389-8c54-cb98f6bb6bf7",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Add performance metrics evaluted on the test set to `wandb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754b9b6-38ef-4c2b-970b-80900633eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time, sys, os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# my modules\n",
    "from sn_camels.models.models_factory import baseModelFactory, topModelFactory\n",
    "from sn_camels.models.sn_hybrid_models import sn_HybridModel\n",
    "from sn_camels.camels.camels_dataset import *\n",
    "from sn_camels.models.camels_models import get_architecture\n",
    "from sn_camels.utils.test_model import test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134c3a2-a71e-49be-a0df-c143c63fcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Base script to test a scattering network on a CAMELs dataset \"\"\"\n",
    "\n",
    "epochs=200\n",
    "lr=0.0015398962166420919\n",
    "batch_size=128\n",
    "project_name=\"new_metrics_debug\"\n",
    "error=True # Predict errors?\n",
    "model_type=\"sn\" ## \"sn\" or \"camels\" for now\n",
    "# hyperparameters\n",
    "wd         = 0.0028321439252305164  #value of weight decay\n",
    "dr         = 0.00675389680089114    #dropout value for fully connected layers\n",
    "hidden     = 12      #this determines the number of channels in the CNNs; integer larger than 1\n",
    "\n",
    "seed       = 1   #random seed to split maps among training, validation and testing\n",
    "splits     = 15   #number of maps per simulation\n",
    "\n",
    "config = {\"learning rate\": lr,\n",
    "                 \"epochs\": epochs,\n",
    "                 \"batch size\": batch_size,\n",
    "                 \"network\": model_type,\n",
    "                 \"dropout\": dr,\n",
    "                 \"error\": error,\n",
    "                 \"hidden\": hidden,\n",
    "                 \"wd\": wd,\n",
    "                 \"splits\":splits}\n",
    "\n",
    "## Initialise wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"%s\" % project_name, entity=\"chris-pedersen\",config=config)\n",
    "\n",
    "## Check if CUDA available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda=True\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda=False\n",
    "\n",
    "\n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n",
    "\n",
    "############################## Set up training params #################################################\n",
    "## camels path\n",
    "camels_path=\"/mnt/ceph/users/camels/PUBLIC_RELEASE/CMD/2D_maps/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154154da-7c64-4fb1-af3f-86ac8809922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_B.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_HI.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mcdm.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mgas.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_MgFe.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mstar.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Mtot.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_ne.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_P.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_T.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Vcdm.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Vgas.npy\"\n",
    "# \"/mnt/home/cpedersen/ceph/Data/CAMELS_test/1k_fields/maps_Z.npy\"\n",
    "\n",
    "# data parameters\n",
    "fmaps      = [\"/mnt/home/cpedersen/ceph/Data/CAMELS_test/15k_fields/maps_Mcdm.npy\"       \n",
    "             ] #tuple containing the maps with the different fields to consider\n",
    "fmaps_norm = [None] #if you want to normalize the maps according to the properties of some data set, put that data set here (This is mostly used when training on IllustrisTNG and testing on SIMBA, or vicerversa)\n",
    "fparams    = camels_path+\"/params_IllustrisTNG.txt\"\n",
    "\n",
    "# training parameters\n",
    "channels        = len(fmaps)                #we only consider here 1 field\n",
    "params          = [0,1,2,3,4,5]    #0(Omega_m) 1(sigma_8) 2(A_SN1) 3 (A_AGN1) 4(A_SN2) 5(A_AGN2). The code will be trained to predict all these parameters.\n",
    "g               = params           #g will contain the mean of the posterior\n",
    "h               = [6+i for i in g] #h will contain the variance of the posterior\n",
    "rot_flip_in_mem = False            #whether rotations and flipings are kept in memory. True will make the code faster but consumes more RAM memory.\n",
    "\n",
    "## Set number of classes for scattering network to output\n",
    "if error==True:\n",
    "    sn_classes=12\n",
    "else:\n",
    "    sn_classes=6\n",
    "\n",
    "# optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "#######################################################################################################\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7108639-060a-4f21-a92a-e2eaeb383284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training set\n",
    "print('\\nPreparing training set')\n",
    "train_loader = create_dataset_multifield('train', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=rot_flip_in_mem, verbose=True)\n",
    "\n",
    "# get validation set\n",
    "print('\\nPreparing validation set')\n",
    "valid_loader = create_dataset_multifield('valid', seed, fmaps, fparams, batch_size, splits, fmaps_norm, \n",
    "                                         rot_flip_in_mem=rot_flip_in_mem,  verbose=True)    \n",
    "\n",
    "# get test set\n",
    "print('\\nPreparing test set')\n",
    "test_loader = create_dataset_multifield('test', seed, fmaps, fparams, batch_size, splits, fmaps_norm,\n",
    "                                         rot_flip_in_mem=rot_flip_in_mem,  verbose=True)\n",
    "\n",
    "num_train_maps=len(train_loader.dataset.x)\n",
    "wandb.config.update({\"no. training maps\": num_train_maps,\n",
    "                     \"fields\": fmaps})\n",
    "\n",
    "if model_type==\"sn\":\n",
    "    ## First create a scattering network object\n",
    "    scatteringBase = baseModelFactory( #creat scattering base model\n",
    "        architecture='scattering',\n",
    "        J=2,\n",
    "        N=256,\n",
    "        M=256,\n",
    "        channels=channels,\n",
    "        max_order=2,\n",
    "        initialization=\"Tight-Frame\",\n",
    "        seed=234,\n",
    "        learnable=False,\n",
    "        lr_orientation=0.005,\n",
    "        lr_scattering=0.005,\n",
    "        skip=True,\n",
    "        split_filters=True,\n",
    "        filter_video=False,\n",
    "        subsample=4,\n",
    "        device=device,\n",
    "        use_cuda=use_cuda\n",
    "    )\n",
    "\n",
    "    ## Now create a network to follow the scattering layers\n",
    "    ## can be MLP, linear, or cnn at the moment\n",
    "    ## (as in https://github.com/bentherien/ParametricScatteringNetworks/ )\n",
    "    top = topModelFactory( #create cnn, mlp, linearlayer, or other\n",
    "        base=scatteringBase,\n",
    "        architecture=\"cnn\",\n",
    "        num_classes=sn_classes,\n",
    "        width=2,\n",
    "        average=False,\n",
    "        use_cuda=use_cuda\n",
    "    )\n",
    "\n",
    "    ## Merge these into a hybrid model\n",
    "    hybridModel = sn_HybridModel(scatteringBase=scatteringBase, top=top, use_cuda=use_cuda)\n",
    "    model=hybridModel\n",
    "    wandb.config.update({\"learnable\":scatteringBase.learnable,\n",
    "                         \"initialisation\":scatteringBase.initialization,\n",
    "                         \"wavelet seed\":scatteringBase.seed,\n",
    "                         \"learnable_parameters\":model.countLearnableParams(),\n",
    "                         \"max_order\":scatteringBase.max_order,\n",
    "                         \"skip\":scatteringBase.skip,\n",
    "                         \"split_filters\":scatteringBase.split_filters,\n",
    "                         \"subsample\":scatteringBase.subsample,\n",
    "                         \"scattering_output_dims\":scatteringBase.M_coefficient,\n",
    "                         \"n_coefficients\":scatteringBase.n_coefficients,\n",
    "                         \"top_model\":top.arch,\n",
    "                         \"spatial_average\":top.average\n",
    "                         })\n",
    "    print(\"scattering layer + cnn set up\")\n",
    "else:\n",
    "    print(\"setting up model %s\" % model_type)\n",
    "    model = get_architecture(model_type,hidden,dr,channels)\n",
    "    wandb.config.update({\"learnable_parameters\":sum(p.numel() for p in model.parameters())})\n",
    "model.to(device=device)\n",
    "\n",
    "# wandb\n",
    "wandb.watch(model, log_freq=1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "\n",
    "# do a loop over all epochs\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    log_dic={}\n",
    "    if model_type==\"sn\":\n",
    "        wave_params=hybridModel.scatteringBase.params_filters\n",
    "        orientations=wave_params[0].cpu().detach().numpy()\n",
    "        xis=wave_params[1].cpu().detach().numpy()\n",
    "        sigmas=wave_params[2].cpu().detach().numpy()\n",
    "        slants=wave_params[3].cpu().detach().numpy()\n",
    "        for aa in range(len(orientations)):\n",
    "            log_dic[\"orientation_%d\" % aa]=orientations[aa]\n",
    "            log_dic[\"xi_%d\" % aa]=xis[aa]\n",
    "            log_dic[\"sigma_%d\" % aa]=sigmas[aa]\n",
    "            log_dic[\"slant_%d\" % aa]=slants[aa]\n",
    "\n",
    "    # train\n",
    "    train_loss1, train_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    train_loss, points = 0.0, 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        bs   = x.shape[0]         #batch size\n",
    "        x    = x.to(device)       #maps\n",
    "        y    = y.to(device)[:,g]  #parameters\n",
    "        p    = model(x)           #NN output\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if error==True:\n",
    "            e_NN = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "            train_loss2 += loss2*bs\n",
    "        else:\n",
    "            loss = torch.mean(torch.log(loss1))\n",
    "        train_loss1 += loss1*bs\n",
    "        points      += bs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.log(train_loss1/points) \n",
    "    if error==True:\n",
    "        train_loss+=torch.log(train_loss2/points)\n",
    "    train_loss = torch.mean(train_loss).item()\n",
    "\n",
    "    # do validation: cosmo alone & all params\n",
    "    valid_loss1, valid_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    valid_loss, points = 0.0, 0\n",
    "    model.eval()\n",
    "    for x, y in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            x     = x.to(device)       #maps\n",
    "            y     = y.to(device)[:,g]  #parameters\n",
    "            p     = model(x)           #NN output\n",
    "            y_NN  = p[:,g]             #posterior mean\n",
    "            loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "            if error==True:    \n",
    "                e_NN  = p[:,h]         #posterior std\n",
    "                loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "                valid_loss2 += loss2*bs\n",
    "            valid_loss1 += loss1*bs\n",
    "            points     += bs\n",
    "\n",
    "    \n",
    "    valid_loss = torch.log(valid_loss1/points) \n",
    "    if error==True:\n",
    "        valid_loss+=torch.log(valid_loss2/points)\n",
    "    valid_loss = torch.mean(valid_loss).item()\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "    log_dic[\"training_loss\"]=train_loss\n",
    "    log_dic[\"valid_loss\"]=valid_loss\n",
    "    wandb.log(log_dic)\n",
    "            \n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_loss, valid_loss), end='')\n",
    "    print(\"\")\n",
    "\n",
    "stop = time.time()\n",
    "print('Time take (h):', \"{:.4f}\".format((stop-start)/3600.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aae2cb-f835-4db1-94a9-de25c6352217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model performance metrics on test set\n",
    "num_maps=test_loader.dataset.size\n",
    "## Now loop over test set and print accuracy\n",
    "# define the arrays containing the value of the parameters\n",
    "params_true = np.zeros((num_maps,6), dtype=np.float32)\n",
    "params_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "errors_NN   = np.zeros((num_maps,6), dtype=np.float32)\n",
    "\n",
    "# get test loss\n",
    "g = [0, 1, 2, 3, 4, 5]\n",
    "test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "test_loss, points = 0.0, 0\n",
    "model.eval()\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        bs    = x.shape[0]    #batch size\n",
    "        x     = x.to(device)  #send data to device\n",
    "        y     = y.to(device)  #send data to device\n",
    "        p     = model(x)      #prediction for mean and variance\n",
    "        y_NN  = p[:,:6]       #prediction for mean\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        if error==True:\n",
    "            e_NN  = p[:,h]         #posterior std\n",
    "            loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "            test_loss2 += loss2*bs\n",
    "        test_loss1 += loss1*bs\n",
    "        test_loss = torch.log(test_loss1/points)\n",
    "        if error==True:\n",
    "            test_loss+=torch.log(test_loss2/points)\n",
    "        test_loss = torch.mean(test_loss).item()\n",
    "\n",
    "        #e_NN  = p[:,6:]       #prediction for error\n",
    "        #loss1 = torch.mean((y_NN[:,g] - y[:,g])**2,                     axis=0)\n",
    "        #loss2 = torch.mean(((y_NN[:,g] - y[:,g])**2 - e_NN[:,g]**2)**2, axis=0)\n",
    "        #test_loss1 += loss1*bs\n",
    "        #test_loss2 += loss2*bs\n",
    "\n",
    "        # save results to their corresponding arrays\n",
    "        params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "        params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "        if error==True:\n",
    "            errors_NN[points:points+x.shape[0]]   = e_NN.cpu().numpy()\n",
    "        points    += x.shape[0]\n",
    "test_loss = torch.log(test_loss1/points) + torch.log(test_loss2/points)\n",
    "test_loss = torch.mean(test_loss).item()\n",
    "print('Test loss = %.3e\\n'%test_loss)\n",
    "\n",
    "# de-normalize\n",
    "## I guess these are the hardcoded parameter limits\n",
    "minimum = np.array([0.1, 0.6, 0.25, 0.25, 0.5, 0.5])\n",
    "maximum = np.array([0.5, 1.0, 4.00, 4.00, 2.0, 2.0])\n",
    "params_true = params_true*(maximum - minimum) + minimum\n",
    "params_NN   = params_NN*(maximum - minimum) + minimum\n",
    "\n",
    "\n",
    "test_error = 100*np.mean(np.sqrt((params_true - params_NN)**2)/params_true,axis=0)\n",
    "print('Error Omega_m = %.3f'%test_error[0])\n",
    "print('Error sigma_8 = %.3f'%test_error[1])\n",
    "print('Error A_SN1   = %.3f'%test_error[2])\n",
    "print('Error A_AGN1  = %.3f'%test_error[3])\n",
    "print('Error A_SN2   = %.3f'%test_error[4])\n",
    "print('Error A_AGN2  = %.3f\\n'%test_error[5])\n",
    "\n",
    "wandb.run.summary[\"Error Omega_m\"]=test_error[0]\n",
    "wandb.run.summary[\"Error sigma_8\"]=test_error[1]\n",
    "wandb.run.summary[\"Error A_SN1\"]  =test_error[2]\n",
    "wandb.run.summary[\"Error A_AGN1\"] =test_error[3]\n",
    "wandb.run.summary[\"Error A_SN2\"]  =test_error[4]\n",
    "wandb.run.summary[\"Error A_AGN2\"] =test_error[5]\n",
    "\n",
    "if error:\n",
    "    errors_NN   = errors_NN*(maximum - minimum)\n",
    "    mean_error = 100*(np.absolute(np.mean(errors_NN/params_NN, axis=0)))\n",
    "    print('Bayesian error Omega_m = %.3f'%mean_error[0])\n",
    "    print('Bayesian error sigma_8 = %.3f'%mean_error[1])\n",
    "    print('Bayesian error A_SN1   = %.3f'%mean_error[2])\n",
    "    print('Bayesian error A_AGN1  = %.3f'%mean_error[3])\n",
    "    print('Bayesian error A_SN2   = %.3f'%mean_error[4])\n",
    "    print('Bayesian error A_AGN2  = %.3f\\n'%mean_error[5])\n",
    "    wandb.run.summary[\"Predicted error Omega_m\"]=mean_error[0]\n",
    "    wandb.run.summary[\"Predicted error sigma_8\"]=mean_error[1]\n",
    "    wandb.run.summary[\"Predicted error A_SN1\"]  =mean_error[2]\n",
    "    wandb.run.summary[\"Predicted error A_AGN1\"] =mean_error[3]\n",
    "    wandb.run.summary[\"Predicted error A_SN2\"]  =mean_error[4]\n",
    "    wandb.run.summary[\"Predicted error A_AGN2\"] =mean_error[5]\n",
    "\n",
    "f, axarr = plt.subplots(3, 2, figsize=(14,20))\n",
    "for aa in range(0,6,2):\n",
    "    axarr[aa//2][0].plot(np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),np.linspace(min(params_true[:,aa]),max(params_true[:,aa]),100),color=\"black\")\n",
    "    axarr[aa//2][1].plot(np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),np.linspace(min(params_true[:,aa+1]),max(params_true[:,aa+1]),100),color=\"black\")\n",
    "    if error==True:\n",
    "        axarr[aa//2][0].errorbar(params_true[:,aa],params_NN[:,aa],errors_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "        axarr[aa//2][1].errorbar(params_true[:,aa+1],params_NN[:,aa+1],errors_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "    else:\n",
    "        axarr[aa//2][0].plot(params_true[:,aa],params_NN[:,aa],marker=\"o\",ls=\"none\")\n",
    "        axarr[aa//2][1].plot(params_true[:,aa+1],params_NN[:,aa+1],marker=\"o\",ls=\"none\")\n",
    "    \n",
    "axarr[0][0].set_xlabel(r\"True $\\Omega_m$\")\n",
    "axarr[0][0].set_ylabel(r\"Predicted $\\Omega_m$\")\n",
    "axarr[0][0].text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr[0][0].transAxes)\n",
    "\n",
    "axarr[0][1].set_xlabel(r\"True $\\sigma_8$\")\n",
    "axarr[0][1].set_ylabel(r\"Predicted $\\sigma_8$\")\n",
    "axarr[0][1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[0][1].transAxes)\n",
    "\n",
    "axarr[1][0].set_xlabel(r\"True $A_\\mathrm{SN1}$\")\n",
    "axarr[1][0].set_ylabel(r\"Predicted $A_\\mathrm{SN1}$\")\n",
    "axarr[1][0].text(0.1,0.9,\"%.3f %% error\" % test_error[2],fontsize=12,transform=axarr[1][0].transAxes)\n",
    "\n",
    "axarr[1][1].set_xlabel(r\"True $A_\\mathrm{AGN1}$\")\n",
    "axarr[1][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN1}$\")\n",
    "axarr[1][1].text(0.1,0.9,\"%.3f %% error\" % test_error[3],fontsize=12,transform=axarr[1][1].transAxes)\n",
    "\n",
    "axarr[2][0].set_xlabel(r\"True $A_\\mathrm{SN2}$\")\n",
    "axarr[2][0].set_ylabel(r\"Predicted $A_\\mathrm{SN2}$\")\n",
    "axarr[2][0].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][0].transAxes)\n",
    "\n",
    "axarr[2][1].set_xlabel(r\"True $A_\\mathrm{AGN2}$\")\n",
    "axarr[2][1].set_ylabel(r\"Predicted $A_\\mathrm{AGN2}$\")\n",
    "axarr[2][1].text(0.1,0.9,\"%.3f %% error\" % test_error[4],fontsize=12,transform=axarr[2][1].transAxes)\n",
    "\n",
    "figure=wandb.Image(f)\n",
    "wandb.log({\"performance\": figure})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f40c6-53c2-40cd-b56e-aae7a77d9c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e64ac4-9bfe-40b7-8bdc-d9d66a658a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavelet",
   "language": "python",
   "name": "wavelet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
